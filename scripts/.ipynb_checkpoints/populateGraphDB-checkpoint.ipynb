{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate RDF database\n",
    "\n",
    "\n",
    "\n",
    "To measure execution time in Jupyter notebooks: <code>pip install ipython-autotime</code>\n",
    "\n",
    "We need to install <code>RDFLib</code>\n",
    "\n",
    "<code>pip3 install rdflib </code> [Documentation](https://rdflib.readthedocs.io/en/stable/gettingstarted.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load files from: /locale/data/jupyter/prando/notebook/2022/notebook_evaluated\n",
      "Successfully load the json notebooks from the folder /locale/data/jupyter/prando/notebook/2022/notebook_evaluated:\n",
      "--> Total number of files: 123\n"
     ]
    }
   ],
   "source": [
    "# folder where JSON files are stored\n",
    "folder = \"/locale/data/jupyter/prando/wd-project/2022/notebook_evaluated\"\n",
    "def load_json(directory,verbose = False):\n",
    "    if verbose:\n",
    "        print(\"Start to load files from:\",directory)\n",
    "    files={}\n",
    "    for folder in os.listdir(directory):\n",
    "        subdir = directory + os.sep + folder\n",
    "        nbks = []\n",
    "        for file in os.listdir(subdir):\n",
    "            filepath = subdir + os.sep + file\n",
    "            if filepath.endswith(\".json\"):\n",
    "                fd = open(filepath,\"r\")\n",
    "                tmp = json.load(fd)\n",
    "                files[tmp[\"name\"]] = tmp\n",
    "                fd.close()\n",
    "    if verbose:\n",
    "        string = \"Successfully load the json notebooks from the folder \"+str(directory)+\":\\n\"\n",
    "        string += \"--> Total number of files: \"+str(len(files))\n",
    "        print(string)\n",
    "    return files\n",
    "\n",
    "files = load_json(folder,True)\n",
    "\n",
    "# saving folder\n",
    "savePath =  '/locale/data/jupyter/prando/notebook/2022/rdf/'\n",
    "#create rdf dir\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF,BNode, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like XSD\n",
    "from rdflib.namespace import XSD\n",
    "\n",
    "# Construct the country and the aitraffic ontology namespaces not known by RDFlib\n",
    "WDO = Namespace(\"http://www.dei.unipd.it/exploratory#\")\n",
    "LSQV = Namespace(\"http://lsq.aksw.org/vocab#\")\n",
    "SP = Namespace(\"http://spinrdf.org/sp#\")\n",
    "DCT = Namespace(\"http://purl.org/dc/terms/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics and Tasks\n",
    "In this section we model the topics and the tasks, creating the turtle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph for the topics and tasks\n",
    "g = Graph()\n",
    "\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"wdo\", WDO)\n",
    "g.bind(\"lsqv\", LSQV)\n",
    "g.bind(\"sp\", SP)\n",
    "\n",
    "#create the graph for the search workflows\n",
    "h = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "h.bind(\"xsd\", XSD)\n",
    "h.bind(\"wdo\", WDO)\n",
    "h.bind(\"lsqv\", LSQV)\n",
    "h.bind(\"sp\", SP)\n",
    "h.bind(\"dct\",DCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "topics=[]\n",
    "workers = []\n",
    "\n",
    "# create the URI for the Completeness Track\n",
    "Track = URIRef(WDO[\"CompletenessTrack\"])\n",
    "g.add((Track, RDF.type, WDO['Track']))\n",
    "g.add((Track, WDO['description'], Literal(\"Completeness Track\", datatype=XSD.string)))\n",
    "index=0\n",
    "executions = 0\n",
    "for filename in files:\n",
    "    topic = files[filename]['topic']\n",
    "    name = files[filename]['name']\n",
    "    worker = files[filename]['student']\n",
    "    macro_topic = files[filename]['macro_topic']\n",
    "    hash_topic = hashlib.md5(topic.encode()).hexdigest()[:10]\n",
    "    # create the URI for the current topic\n",
    "    Topic = URIRef(WDO[\"TOPIC\"+hash_topic])\n",
    "    # add the topic with all the tasks to the graph G\n",
    "    topics.append(topic)\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Topic, RDF.type, WDO['SearchTopic']))\n",
    "    # add the description\n",
    "    g.add((Topic, WDO['description'], Literal(topic, datatype=XSD.string)))\n",
    "    # add the macro topic\n",
    "    g.add((Topic, WDO['macroTopic'], Literal(macro_topic, datatype=XSD.string)))\n",
    "    # add the link to the Track\n",
    "    g.add((Topic, WDO['partOf'], Track))\n",
    "    # add the search tasks\n",
    "    goals = files[filename]['goals']\n",
    "    # keep the URI of the Tasks saved to use later\n",
    "    tasks = {}\n",
    "    for goal in goals:\n",
    "        no_hashed_goal = topic+goal\n",
    "        hash_goal = hashlib.md5(no_hashed_goal.encode()).hexdigest()[:10]\n",
    "        Task = URIRef(WDO[\"TASK\"+str(hash_goal)])\n",
    "        tasks[goal] = Task\n",
    "        ## add the Tasks\n",
    "        g.add((Task, RDF.type, WDO['SearchTask']))\n",
    "        g.add((Task, WDO['description'], Literal(goals[goal], datatype=XSD.string)))\n",
    "        g.add((Task, WDO['number'], Literal(str(goal), datatype=XSD.string)))\n",
    "        g.add((Task, WDO['belongsTo'], Topic))\n",
    "    \n",
    "    \n",
    "    ## add the search workflow\n",
    "    Workflow = URIRef(WDO[name])\n",
    "    h.add((Workflow, RDF.type, WDO['SearchWorkflow']))\n",
    "    # add the related topic\n",
    "    h.add((Workflow, WDO['implements'], Topic))\n",
    "    \n",
    "    #create the Worker\n",
    "    Worker = URIRef(WDO[\"WORKER\"+str(worker)])\n",
    "    h.add((Workflow, WDO['wroteBy'], Worker))\n",
    "    \n",
    "    if worker not in workers:\n",
    "        ## add the Worker\n",
    "        workers.append(worker)\n",
    "        h.add((Worker, RDF.type, WDO['Worker']))\n",
    "        ## add also the score of the worker given is exam score\n",
    "    \n",
    "    search_workflow = files[filename]['search_workflow']\n",
    "    for job in search_workflow:\n",
    "        # the Job's URI is the concatenation of [JOB, number of the task, W, name of the file]\n",
    "        Job = URIRef(WDO['JOB'+str(job)+'W'+name])\n",
    "        h.add((Job, RDF.type, WDO['SearchJob']))\n",
    "        # add the relation hasPart to the search workflow\n",
    "        h.add((Workflow, WDO['hasPart'], Job))\n",
    "        # add the relation performs to the search task\n",
    "        h.add((Job, WDO['performs'], tasks[goal]))\n",
    "        # add the query list\n",
    "        Queries = BNode()\n",
    "        h.add((Queries, RDF.type, RDF.List))\n",
    "        h.add((Job, WDO['queries'], Queries))\n",
    "        \n",
    "        \n",
    "        ## create the list of the query\n",
    "        \n",
    "        for i in range(len(search_workflow[job])):\n",
    "            query = search_workflow[job][i]\n",
    "            narrative = query['narrative']\n",
    "            text = query['query']\n",
    "            Query = URIRef(SP['Q'+str(index)])\n",
    "            h.add((Query, RDF.type, SP['Query']))\n",
    "            h.add((Query, SP['text'], Literal(text, datatype=XSD.string)))\n",
    "            # add the parse Error if it exists\n",
    "            if 'parseError' in query and query['parseError'] is not None:\n",
    "                h.add((Query, LSQV['parseError'], Literal(query['parseError'], datatype=XSD.string)))\n",
    "            # add the narrative if it exists\n",
    "            if 'narrative' in query and query['narrative'] is not None:\n",
    "                h.add((Query, WDO['narrative'], Literal(query['narrative'], datatype=XSD.string)))\n",
    "            # add the size of the result set if it exists\n",
    "            if 'output' in query and query['output'] is not None:\n",
    "                h.add((Query, LSQV['resultSize'], Literal(str(len(query['output'])), datatype=XSD.long)))\n",
    "                \n",
    "            # add the metrics\n",
    "            metrics = ['recall','precision','accuracy']\n",
    "            for m in metrics:\n",
    "                if m in query and query[m] is not None:\n",
    "                    h.add((Query, WDO[m], Literal(query[m]), datatype=XSD.float))\n",
    "            # add the executions\n",
    "            # example: \"22/Dec/2022:19:41:16\"\n",
    "            for ex in query['execution']:\n",
    "                t_ex = datetime.strptime(ex,'%d/%b/%Y:%H:%M:%S',).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "                Execution = URIRef(DCT['EX'+str(executions)])\n",
    "                h.add((Execution, RDF.type, LSQV['Execution']))\n",
    "                g.add((Query, DCT['issued'], Literal(t_ex, datatype=XSD.dateTime)))\n",
    "                executions+=1\n",
    "                \n",
    "            h.add((Queries, RDF.first, Query))\n",
    "            if i < len(search_workflow[job])-1:\n",
    "                Next = BNode()\n",
    "                h.add((Next, RDF.type, RDF.List))\n",
    "                h.add((Queries, RDF.rest, Next))\n",
    "                Queries = Next\n",
    "            else:\n",
    "                h.add((Queries, RDF.rest, RDF.nil))\n",
    "            index+=1\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print the data for the topics in the Turtle format\n",
    "ttlname = 'topics.ttl'\n",
    "print(\"--- saving serialization for the topics ---\")\n",
    "with open(savePath + ttlname, 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization for the last aircrafts ---\n",
      "CPU times: user 1.52 s, sys: 51.4 ms, total: 1.57 s\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print the data for the workfflows in the Turtle format\n",
    "ttlname = 'workflows.ttl'\n",
    "print(\"--- saving serialization for the last aircrafts ---\")\n",
    "with open(savePath + ttlname, 'w') as file:\n",
    "    file.write(h.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workers quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2022_0': 0.0, '2022_1': 0.5714285714285714, '2022_2': 1.0, '2022_3': 0.9285714285714286, '2022_4': 0.5714285714285714, '2022_5': 0.5, '2022_6': 0.9285714285714286, '2022_7': 0.0, '2022_8': 0.0, '2022_9': 0.0, '2022_10': 0.9285714285714286, '2022_11': 0.5, '2022_12': 1.0, '2022_13': 0.7857142857142857, '2022_14': 0.0, '2022_15': 0.9285714285714286, '2022_16': 0.6428571428571429, '2022_17': 0.7857142857142857, '2022_18': 0.0, '2022_19': 0.7857142857142857, '2022_20': 0.5714285714285714, '2022_21': 0.0, '2022_22': 1.0, '2022_23': 1.0, '2022_24': 0.0, '2022_25': 0.9285714285714286, '2022_26': 0.7142857142857143, '2022_27': 1.0, '2022_28': 0.7142857142857143, '2022_29': 0.2857142857142857, '2022_30': 0.9285714285714286, '2022_31': 0.7857142857142857, '2022_32': 1.0, '2022_33': 0.0, '2022_34': 0.7142857142857143, '2022_35': 0.9285714285714286, '2022_36': 0.0, '2022_37': 1.0, '2022_38': 0.0, '2022_39': 0.8571428571428571, '2022_40': 0.0, '2021_0': 1.0, '2021_1': 0.9285714285714286, '2021_2': 0.9285714285714286, '2021_3': 0.7857142857142857, '2021_4': 0.5, '2021_5': 1.0, '2021_6': 0.5, '2021_7': 0.42857142857142855, '2021_8': 0.7857142857142857, '2021_9': 0.9285714285714286, '2021_10': 0.9285714285714286, '2021_11': 0.35714285714285715, '2021_12': 0.7142857142857143, '2021_13': 0.7857142857142857, '2021_14': 0.7857142857142857, '2021_15': 0.7142857142857143, '2021_16': 0.0, '2021_17': 0.5714285714285714, '2021_18': 0.5714285714285714, '2021_19': 0.7857142857142857, '2021_20': 0.9285714285714286}\n"
     ]
    }
   ],
   "source": [
    "f = open(\"../workers.csv\",\"r\")\n",
    "lines = f.read().split(\"\\n\")[1:-1]\n",
    "f.close()\n",
    "workers = {}\n",
    "for w in lines:\n",
    "    spl = w.split(\",\")\n",
    "    name = spl[0]+\"_\"+spl[1]\n",
    "    workers[name] = 0.0\n",
    "    if int(spl[2])!=0:\n",
    "        workers[name] = (float(int(spl[2])-17.0)/14.0)\n",
    "\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic = URIRef(WDO[\"TOPICHHH\"])\n",
    "\n",
    "# Add triples using store's add() method.\n",
    "g.add((Topic, RDF.type, WDO['SearchTopic']))\n",
    "# add the description\n",
    "g.add((Topic, WDO['description'], Literal(topic, datatype=XSD.string)))\n",
    "# print the data for the topics in the Turtle format\n",
    "ttlname = 'topics.ttl'\n",
    "print(\"--- saving serialization for the topics ---\")\n",
    "with open(savePath + ttlname, 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
